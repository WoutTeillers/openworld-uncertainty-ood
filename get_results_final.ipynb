{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d035c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\Wout Teillers\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'open-images-v7-validation-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading config file fiftyone.yml from Voxel51/LVIS\n",
      "Loading dataset\n",
      "Ignoring unsupported parameter 'splits' for importer type <class 'fiftyone.utils.data.importers.FiftyOneDatasetImporter'>\n",
      "Importing samples...\n",
      " 100% |███████████████| 1000/1000 [246.4ms elapsed, 0s remaining, 4.1K samples/s]      \n",
      "Migrating dataset 'Voxel51/LVIS' to v1.3.0\n",
      " 100% |███████████████████| 39/39 [218.1ms elapsed, 0s remaining, 178.8 samples/s] \n"
     ]
    }
   ],
   "source": [
    "# First, load the extension (only need to do this once per session)\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to automatically reload modules before executing any code\n",
    "%autoreload 2\n",
    "\n",
    "from src.result_shower import ResultShower\n",
    "resultshower = ResultShower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134c398",
   "metadata": {},
   "source": [
    "### list of all incorrect filenames for the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_missing_lvis = [\n",
    "(\"000000192722.jpg\", ['footwear']),\n",
    "(\"000000569652.jpg\", ['power plugs and sockets']),\n",
    "(\"000000452334.jpg\", ['home appliance', 'kitchen appliance']),\n",
    "(\"000000006777.jpg\", ['stairs']),\n",
    "(\"000000269417.jpg\", ['kitchen appliance']),\n",
    "(\"000000394879.jpg\", ['coffeemaker', 'dessert']),\n",
    "(\"000000375317.jpg\", ['gas stove', 'home appliance']),\n",
    "(\"000000244965.jpg\", [\"mammal\"]),\n",
    "(\"000000125247.jpg\", ['candy']),\n",
    "(\"000000090122.jpg\", ['kitchen utensil']),\n",
    "(\"000000175205.jpg\", ['dairy product']),\n",
    "(\"000000290911.jpg\", ['vehicle']),\n",
    "(\"000000497875.jpg\", ['tree']),\n",
    "(\"000000424044.jpg\", ['baked goods', 'mixing bowl']),\n",
    "(\"000000216863.jpg\", ['picture frame']),\n",
    "(\"000000051618.jpg\", ['tableware']),\n",
    "(\"000000263589.jpg\", ['building']),\n",
    "(\"000000044611.jpg\", ['trousers']),\n",
    "(\"000000048432.jpg\", ['land vehicle']),\n",
    "(\"000000277858.jpg\", ['mammal']),\n",
    "(\"000000205055.jpg\", ['building', 'footwear']),\n",
    "(\"000000406013.jpg\", ['plastic bag']),\n",
    "(\"000000534751.jpg\", ['furniture']),\n",
    "(\"000000244157.jpg\", ['tree']),\n",
    "(\"000000094052.jpg\", ['microwave oven', 'picture frame', 'food']),\n",
    "(\"000000471842.jpg\", ['kitchenware']),\n",
    "(\"000000261893.jpg\", ['bicycle wheel']),\n",
    "(\"000000364210.jpg\", ['baked goods']),\n",
    "(\"000000442298.jpg\", ['furniture', 'mammal']),\n",
    "(\"000000468917.jpg\", ['microwave oven']),\n",
    "(\"000000362140.jpg\", ['fashion accessory', 'picture frame']),\n",
    "(\"000000555273.jpg\", ['picture frame']),\n",
    "(\"000000127100.jpg\", ['footwear']),\n",
    "(\"000000372980.jpg\", ['mammal']),\n",
    "(\"000000043692.jpg\", ['countertop', 'furniture']),\n",
    "(\"000000315902.jpg\", ['furniture']),\n",
    "(\"000000505152.jpg\", ['kitchen utensil']),\n",
    "(\"000000223032.jpg\", ['mammal']),\n",
    "(\"000000070164.jpg\", ['remote control']),\n",
    "(\"000000356153.jpg\", ['food']),\n",
    "(\"000000334352.jpg\", ['sports equipment']),\n",
    "(\"000000455691.jpg\", ['countertop']),\n",
    "(\"000000526794.jpg\", ['home appliance']),\n",
    "(\"000000086208.jpg\", ['snack']),\n",
    "(\"000000103223.jpg\", ['home appliance']),\n",
    "(\"000000501247.jpg\", ['land vehicle']),\n",
    "(\"000000160142.jpg\", ['furniture']),\n",
    "(\"000000521200.jpg\", ['hand dryer']),\n",
    "(\"000000040930.jpg\", ['window']),\n",
    "(\"000000120527.jpg\", ['sports equipment']),\n",
    "(\"000000460442.jpg\", ['fruit', 'footwear']),\n",
    "(\"000000423161.jpg\", ['flower']),\n",
    "(\"000000053037.jpg\", ['coffeemaker', 'microwave oven']),\n",
    "(\"000000218751.jpg\", ['land vehicle']),\n",
    "(\"000000361497.jpg\", ['aircraft']),\n",
    "(\"000000278303.jpg\", ['mammal']),\n",
    "(\"000000095841.jpg\", ['footwear']),\n",
    "(\"000000018090.jpg\", ['footwear']),\n",
    "(\"000000457737.jpg\", ['furniture']),\n",
    "(\"000000404698.jpg\", ['power plugs and sockets']),\n",
    "(\"000000442298.jpg\", ['furniture', 'mammal']),\n",
    "(\"000000513604.jpg\", ['tin can']),\n",
    "(\"000000019441.jpg\", ['tableware']),\n",
    "(\"000000442875.jpg\", ['furniture']),\n",
    "(\"000000007288.jpg\", ['bidet']),\n",
    "(\"000000071726.jpg\", ['soap dispenser', 'cabinetry'])]\n",
    "\n",
    "incorrect_missing_OI = [\n",
    "    (\"00141571d986d241.jpg\", ['hand_towel', 't-shirt']), \n",
    "    (\"00146ba1e50ed8d8.jpg\", ['cylinder']), \n",
    "    (\"0035c28612c035fd.jpg\", ['green_bean']), \n",
    "    (\"00acf53b127218c2.jpg\", ['radiator_grille']), \n",
    "    (\"00dc0530e6779ca6.jpg\", ['baby_buggy']), \n",
    "    (\"01491bf840ae9939.jpg\", ['activewear']), \n",
    "    (\"015f5cd905204962.jpg\", ['trousers']), \n",
    "    (\"0197df7725980004.jpg\", ['rearview_mirror']), \n",
    "    (\"01b405e0cab3add3.jpg\", ['baseball_cap']), \n",
    "    (\"01f26ca52e27a8d9.jpg\", ['pencil_case']), \n",
    "    (\"023a57536e17b7b1.jpg\", ['figurine']), \n",
    "    (\"025ffa27eb2ba851.jpg\", ['printing_machine']), \n",
    "    (\"030033e1b4137e3b.jpg\", ['dog_collar']), \n",
    "    (\"03650b9fde97f523.jpg\", ['wristwatch']), \n",
    "    (\"049720d842de2d3e.jpg\", ['paper_towel']), \n",
    "    (\"04d9284ebdc41aeb.jpg\", ['cordial']), \n",
    "    (\"04ec0b057014a648.jpg\", ['jockey_cap']), \n",
    "    (\"006f87bf928f9ba3.jpg\", ['jewellery']), \n",
    "    (\"00c9616a917be867.jpg\", ['fin_(footwear)']), \n",
    "    (\"01c79b8cc239037d.jpg\", ['wedding_ring']), \n",
    "    (\"038ee0bf31929792.jpg\", ['flip-flop_(sandal)']), \n",
    "    (\"05d69a9470032674.jpg\", ['sport_shirt'])\n",
    "]   \n",
    "\n",
    "dict_incorrect_missing_lvis = dict(incorrect_missing_lvis)\n",
    "dict_incorrect_missing_oi = dict(incorrect_missing_OI)\n",
    "\n",
    "print(f'len of missing LVIS = {len(dict_incorrect_missing_lvis)}')\n",
    "print(f'len of missing Open Images = {len(dict_incorrect_missing_oi)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1223c6",
   "metadata": {},
   "source": [
    "### Load the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940210ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_yolo_lvis, dataset_yolo_openimages, dataset_yolo_japan = resultshower.load_yolo_datasets()\n",
    "dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan = resultshower.load_dino_datasets()\n",
    "dataset_gpt_lvis, dataset_gpt_openimages, dataset_gpt_japan = resultshower.load_gpt_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ec256",
   "metadata": {},
   "source": [
    "### Filter dino predictions by filtering combination labels and applying NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d298a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.remove_combi_predictions_dino(dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan)\n",
    "resultshower.remove_overlapping_prediction_with_nms(dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebf0cd",
   "metadata": {},
   "source": [
    "### Delete incorrect missing labels from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.filter_missing_predictions(dataset_yolo_lvis, dict_incorrect_missing_lvis)\n",
    "resultshower.filter_missing_predictions(dataset_dino_lvis, dict_incorrect_missing_lvis)\n",
    "resultshower.filter_missing_predictions(dataset_gpt_lvis, dict_incorrect_missing_lvis)\n",
    "\n",
    "resultshower.filter_missing_predictions(dataset_yolo_openimages, dict_incorrect_missing_oi)\n",
    "resultshower.filter_missing_predictions(dataset_dino_openimages, dict_incorrect_missing_oi)\n",
    "resultshower.filter_missing_predictions(dataset_gpt_openimages, dict_incorrect_missing_oi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5c7b6",
   "metadata": {},
   "source": [
    "### Make list of models on each individual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_datasets_individual_models = [dataset_yolo_lvis, dataset_yolo_openimages, dataset_yolo_japan, \n",
    "                                 dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan, \n",
    "                                 dataset_gpt_lvis, dataset_gpt_openimages, dataset_gpt_japan]\n",
    "\n",
    "li_datasets_individual_models_names = ['YOLO-World LVIS', 'YOLO-World Open Images', 'YOLO-World JUS', \n",
    "                                       'Grounding DINO LVIS', 'Grounding DINO Open Images', 'Grounding DINO JUS', \n",
    "                                       'GPT-4o LVIS', 'GPT-4o Open Images', 'GPT-4o JUS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcfb94",
   "metadata": {},
   "source": [
    "### plot correlations of size of bounding boxes with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.plot_correlation(li_datasets=li_datasets_individual_models, \n",
    "                              li_names=li_datasets_individual_models_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20f724",
   "metadata": {},
   "source": [
    "### plot density histogram of confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultshower.plot_density_confidences_seperate(li_datasets=li_datasets_individual_models, \n",
    "                                      li_names=li_datasets_individual_models_names,num=2)\n",
    "resultshower.plot_density_confidences_seperate(li_datasets=li_datasets_individual_models, \n",
    "                                      li_names=li_datasets_individual_models_names,num=1)\n",
    "resultshower.plot_density_confidences_seperate(li_datasets=li_datasets_individual_models, \n",
    "                                      li_names=li_datasets_individual_models_names,num=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b88ab2",
   "metadata": {},
   "source": [
    "### Add missing existing labels by assigning 0 confidence with bounding box at (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27887e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [dataset_yolo_lvis, dataset_yolo_openimages, \n",
    "           dataset_dino_lvis, dataset_dino_openimages, \n",
    "           dataset_gpt_lvis, dataset_gpt_openimages]:\n",
    "    \n",
    "    resultshower.give_predictions_if_none(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d53ad",
   "metadata": {},
   "source": [
    "### Evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d84a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.eval_datasets([dataset_yolo_lvis, dataset_yolo_openimages, \n",
    "           dataset_dino_lvis, dataset_dino_openimages, \n",
    "           dataset_gpt_lvis, dataset_gpt_openimages])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d951f",
   "metadata": {},
   "source": [
    "### check the number of correct responses from gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.check_gpt_output(datafiles=['20250417_log_gpt_japan.json', \n",
    "                                         '20250417_log_gpt_open_images.json', \n",
    "                                         '20250417_log_gpt_lvis.json', \n",
    "                                         '20250502_log_gpt_japan.json'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fa10d",
   "metadata": {},
   "source": [
    "### plot a histogram of number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultshower.plot_number_predictions(li_models=li_datasets_individual_models,\n",
    "                                     li_names=li_datasets_individual_models_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0171199",
   "metadata": {},
   "source": [
    "### Scatter plot of number of classes against different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_models = [dataset_yolo_lvis, dataset_yolo_openimages, \n",
    "            dataset_dino_lvis, dataset_dino_openimages,\n",
    "            dataset_gpt_lvis, dataset_gpt_openimages]\n",
    "li_names = ['YOLO-World LVIS', 'YOLO-World Open Images',\n",
    "            'Grounding DINO LVIS', 'Grounding DINO Open Images',\n",
    "            'GPT-4o LVIS', 'GPT-4o Open Images']\n",
    "resultshower.plot_metrics_vs_classes(li_datasets=li_models,\n",
    "                                     li_names=li_names, metric='ECE')\n",
    "resultshower.plot_metrics_vs_classes(li_datasets=li_models,\n",
    "                                     li_names=li_names, metric='mAP')\n",
    "resultshower.plot_metrics_vs_classes(li_datasets=li_models,\n",
    "                                     li_names=li_names, metric='AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d7b46",
   "metadata": {},
   "source": [
    "### Load processed and evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1aec77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1000/1000 [10.2s elapsed, 0s remaining, 164.6 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [508.7ms elapsed, 0s remaining, 2.0K samples/s]      \n",
      "merging done for openimages\n",
      " 100% |███████████████| 1000/1000 [15.6s elapsed, 0s remaining, 63.6 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [849.7ms elapsed, 0s remaining, 1.2K samples/s]       \n",
      "merging done for LVIS\n",
      " 100% |███████████████████| 39/39 [196.2ms elapsed, 0s remaining, 201.2 samples/s] \n",
      " 100% |███████████████████| 39/39 [63.1ms elapsed, 0s remaining, 617.9 samples/s]     \n",
      "merging done for JUS\n",
      " 100% |███████████████| 1000/1000 [28.1s elapsed, 0s remaining, 76.4 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [4.2s elapsed, 0s remaining, 265.3 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [30.9s elapsed, 0s remaining, 31.2 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [6.5s elapsed, 0s remaining, 148.2 samples/s]      \n",
      " 100% |███████████████████| 39/39 [1.2s elapsed, 0s remaining, 33.2 samples/s]         \n",
      " 100% |███████████████████| 39/39 [332.1ms elapsed, 0s remaining, 118.3 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [4.1s elapsed, 0s remaining, 276.0 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [306.0ms elapsed, 0s remaining, 3.3K samples/s]      \n",
      " 100% |███████████████| 1000/1000 [4.2s elapsed, 0s remaining, 245.5 samples/s]      \n",
      " 100% |███████████████| 1000/1000 [370.3ms elapsed, 0s remaining, 2.7K samples/s]      \n",
      " 100% |███████████████████| 39/39 [147.7ms elapsed, 0s remaining, 269.5 samples/s]    \n",
      " 100% |███████████████████| 39/39 [57.1ms elapsed, 0s remaining, 682.5 samples/s]     \n"
     ]
    }
   ],
   "source": [
    "# TO DO LOAD PROCESSED DATASETS\n",
    "\n",
    "# Set to true if you want to load processed data\n",
    "load_processed = True\n",
    "\n",
    "if load_processed:\n",
    "\n",
    "    dir_names = ['yolo_open_images_existing', 'yolo_open_images_missing', \n",
    "                'yolo_lvis_existing', 'yolo_lvis_missing', \n",
    "                'yolo_japan_existing', 'yolo_japan_missing']\n",
    "\n",
    "    dataset_yolo_lvis, dataset_yolo_openimages, dataset_yolo_japan = resultshower.load_yolo_datasets(dir='data_with_eval', dir_names=dir_names)\n",
    "\n",
    "    dir_names = ['dino_open_images_existing', 'dino_open_images_missing', \n",
    "                'dino_lvis_existing', 'dino_lvis_missing', \n",
    "                'dino_japan_existing', 'dino_japan_missing']\n",
    "    dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan = resultshower.load_dino_datasets(dir='data_with_eval', dir_names=dir_names)\n",
    "\n",
    "    dir_names = ['gpt_open_images_existing', 'gpt_open_images_missing', \n",
    "                'gpt_lvis_existing', 'gpt_lvis_missing', \n",
    "                'gpt_japan_existing', 'gpt_japan_missing']\n",
    "    dataset_gpt_lvis, dataset_gpt_openimages, dataset_gpt_japan = resultshower.load_gpt_datasets(dir='data_with_eval', dir_names=dir_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f777b0",
   "metadata": {},
   "source": [
    "### Get combination of datasets for each model (combi of LVIS and Open Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_yolo_eval = resultshower.make_combination_datasets(dataset_yolo_lvis, dataset_yolo_openimages, 'dataset_yolo_eval')\n",
    "dataset_dino_eval = resultshower.make_combination_datasets(dataset_dino_lvis, dataset_dino_openimages, 'dataset_dino_eval')\n",
    "dataset_gpt_eval = resultshower.make_combination_datasets(dataset_gpt_lvis, dataset_gpt_openimages, 'dataset_gpt_eval')\n",
    "\n",
    "li_eval_models = [dataset_yolo_eval, dataset_dino_eval, dataset_gpt_eval]\n",
    "li_eval_models_names = ['YOLO-World', 'Grounding Dino', 'GPT-4o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "li_datasets_individual_models = [dataset_yolo_lvis, dataset_yolo_openimages, \n",
    "                                 dataset_dino_lvis, dataset_dino_openimages, \n",
    "                                 dataset_gpt_lvis, dataset_gpt_openimages]\n",
    "\n",
    "li_datasets_individual_models_names = ['YOLO-World LVIS', 'YOLO-World Open Images', \n",
    "                                       'Grounding DINO LVIS', 'Grounding DINO Open Images', \n",
    "                                       'GPT-4o LVIS', 'GPT-4o Open Images']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f22aa",
   "metadata": {},
   "source": [
    "### plot ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultshower.plot_roc_curves(li_models=li_eval_models,\n",
    "#                              li_names=li_eval_models_names, title='ROC Curve for existing predictions')\n",
    "\n",
    "# resultshower.plot_roc_curves(li_models=li_datasets_individual_models,\n",
    "#                              li_names=li_datasets_individual_models_names, title='ROC Curve for existing predictions ')\n",
    "\n",
    "resultshower.plot_roc_curves(li_models=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis],\n",
    "                             li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='ROC Curve for existing predictions for LVIS')\n",
    "\n",
    "resultshower.plot_roc_curves(li_models=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages],\n",
    "                             li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title = 'ROC Curve for existing predictions for Open Images')\n",
    "\n",
    "resultshower.plot_roc_curves(li_models=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis],\n",
    "                             li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='ROC Curve for existing and missing predictions for LVIS', include_missing=True)\n",
    "\n",
    "resultshower.plot_roc_curves(li_models=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages],\n",
    "                             li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title = 'ROC Curve for existing and missing predictions for Open Images', include_missing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df3d0c",
   "metadata": {},
   "source": [
    "### plot calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e53c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultshower.plot_calibration_plot(li_models=li_eval_models,\n",
    "#                                     li_names=li_eval_models_names, title='Calibration Plot for existing predictions')\n",
    "\n",
    "# resultshower.plot_calibration_plot(li_models=li_datasets_individual_models,\n",
    "#                                    li_names=li_datasets_individual_models_names, title = 'Calibration Plot for existing predictions')\n",
    "\n",
    "resultshower.plot_calibration_plot(li_models=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis],\n",
    "                                   li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='Calibration Plot for existing predictions for LVIS')\n",
    "resultshower.plot_calibration_plot(li_models=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis],\n",
    "                                   li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='Calibration Plot for existing and missing predictions for LVIS', include_missing=True)\n",
    "\n",
    "resultshower.plot_calibration_plot(li_models=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages],\n",
    "                                   li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='Calibration Plot for existing predictions for Open Images')\n",
    "resultshower.plot_calibration_plot(li_models=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages],\n",
    "                                   li_names=['YOLO-World', 'Grounding DINO', 'GPT-4o'], title='Calibration Plot for existing and missing predictions for Open Images', include_missing =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0d5a5",
   "metadata": {},
   "source": [
    "### Get sample_id with specific properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b033a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get sample with most prediction in existing labels across all three datasets\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='detections'))\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='detections'))\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='detections'))\n",
    "print('most predictions existing labels\\n')\n",
    "\n",
    "# get sample with most prediction in missing labels across all three datasets\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='missing_predictions'))\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='missing_predictions'))\n",
    "print(resultshower.get_sample_most_predictions(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='missing_predictions'))\n",
    "print('most predictions missing labels\\n')\n",
    "\n",
    "# get sample with highest average confidence across all three datasets existing predictions\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='detections', highest=True))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='detections', highest=True))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='detections', highest=True))\n",
    "print('highest average existing prediction\\n')\n",
    "\n",
    "# get sample with lowest average confidence across all three datasets existing predictions\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='detections', highest=False))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='detections', highest=False))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='detections', highest=False))\n",
    "print('lowest average existing prediction\\n')\n",
    "\n",
    "# get sample with lowest average confidence across all three datasets missing predictions\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='missing_predictions', highest=False))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='missing_predictions', highest=False))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='missing_predictions', highest=False))\n",
    "print('highest average missing prediction\\n')\n",
    "\n",
    "# get sample with highest average confidence across all three datasets missing predictions\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_lvis, dataset_dino_lvis, dataset_gpt_lvis], label='missing_predictions', highest=True))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages], label='missing_predictions', highest=True))\n",
    "print(resultshower.get_sample_with_extreme_confidence(li_datasets=[dataset_yolo_japan, dataset_dino_japan, dataset_gpt_japan], label='missing_predictions', highest=True))\n",
    "print('lowest average missing prediction\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "\n",
    "def save_dataset(dataset, dir_name, label):\n",
    "    export_dir = os.path.join('data_with_eval', dir_name)\n",
    "\n",
    "    dataset.export(\n",
    "        export_dir=export_dir,\n",
    "        dataset_type=fo.types.COCODetectionDataset,\n",
    "        export_media=True,\n",
    "        label_field = label\n",
    "    )\n",
    "\n",
    "# set to True if dataset have to be exported\n",
    "export = False\n",
    "\n",
    "if export:\n",
    "    # save YOLO-World datasets\n",
    "    save_dataset(dataset_yolo_lvis, dir_name='yolo_lvis_existing', label='detections')\n",
    "    save_dataset(dataset_yolo_lvis, dir_name='yolo_lvis_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_yolo_openimages, dir_name='yolo_open_images_existing', label='detections')\n",
    "    save_dataset(dataset_yolo_openimages, dir_name='yolo_open_images_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_yolo_japan, dir_name='yolo_japan_existing', label='detections')\n",
    "    save_dataset(dataset_yolo_japan, dir_name='yolo_japan_missing', label='missing_predictions')\n",
    "\n",
    "    # save Grounding Dino datasets\n",
    "    save_dataset(dataset_dino_lvis, dir_name='dino_lvis_existing', label='detections')\n",
    "    save_dataset(dataset_dino_lvis, dir_name='dino_lvis_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_dino_openimages, dir_name='dino_open_images_existing', label='detections')\n",
    "    save_dataset(dataset_dino_openimages, dir_name='dino_open_images_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_dino_japan, dir_name='dino_japan_existing', label='detections')\n",
    "    save_dataset(dataset_dino_japan, dir_name='dino_japan_missing', label='missing_predictions')\n",
    "\n",
    "    # save gpt-4o datasets\n",
    "    save_dataset(dataset_gpt_lvis, dir_name='gpt_lvis_existing', label='detections')\n",
    "    save_dataset(dataset_gpt_lvis, dir_name='gpt_lvis_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_gpt_openimages, dir_name='gpt_open_images_existing', label='detections')\n",
    "    save_dataset(dataset_gpt_openimages, dir_name='gpt_open_images_missing', label='missing_predictions')\n",
    "    save_dataset(dataset_gpt_japan, dir_name='gpt_japan_existing', label='detections')\n",
    "    save_dataset(dataset_gpt_japan, dir_name='gpt_japan_missing', label='missing_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1,s2,s3 in zip(dataset_yolo_openimages, dataset_dino_openimages, dataset_gpt_openimages):\n",
    "    if s1.filename == '05d69a9470032674.jpg':\n",
    "        print(s1.id)\n",
    "        print(s2.id)\n",
    "        print(s3.id)\n",
    "        if 'missing_predictions' in s1 and s1['missing_predictions']:\n",
    "            print(list(set([det.label for det in s1.missing_predictions.detections])))\n",
    "        if 'missing_predictions' in s2 and s2['missing_predictions']:\n",
    "            print(list(set([det.label for det in s2.missing_predictions.detections])))\n",
    "        if 'missing_predictions' in s3 and s3['missing_predictions']:\n",
    "            print(list(set([det.label for det in s3.missing_predictions.detections])))        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "best_sample = None\n",
    "for sample in dataset_gpt_lvis:\n",
    "    count = 0\n",
    "    if 'detections' in sample and sample['detections']:\n",
    "        for det in sample.detections.detections:\n",
    "            if det['eval'] == 'fp':\n",
    "                count+=1\n",
    "        if count > max:\n",
    "            max = count\n",
    "            best_sample = sample\n",
    "\n",
    "print(best_sample.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "best_sample = None\n",
    "samples = []\n",
    "for dataset, name in zip([dataset_dino_lvis, dataset_dino_openimages, dataset_dino_japan], ['lvis', 'openimages', 'japan']):\n",
    "    for sample in dataset:\n",
    "        count = 0\n",
    "        if 'missing_predictions' in sample and sample['missing_predictions']:\n",
    "            max = 0\n",
    "            for det in sample.missing_predictions.detections:\n",
    "                det.confidence\n",
    "                \n",
    "                if det.confidence >= max:\n",
    "                    max = det.confidence\n",
    "                    best_sample = sample\n",
    "                    # print(f'sample = {sample.id}, conf = {det.confidence} in dataset: {name}')\n",
    "            samples.append((max, best_sample.id))\n",
    "samples.sort(key=lambda a: a[0], reverse=True)\n",
    "# print(samples)\n",
    "for sample in samples[:30]:\n",
    "    print(sample)\n",
    "# print(best_sample.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fb035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at localhost.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=ba8aaf21-f5b1-48cd-ab85-656345b033b1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x258a83348e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset:          2025.06.16.08.02.21\n",
       "Media type:       image\n",
       "Num samples:      1000\n",
       "Selected samples: 0\n",
       "Selected labels:  0\n",
       "Session URL:      http://localhost:5151/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# filtered_view = dataset_yolo_openimages.filter_labels(\"missing_predictions\", F(\"confidence\") > 0)\n",
    "\n",
    "# Visualize the filtered samples\n",
    "# print(filtered_view.head())  # Adjust as needed for visualization or further processin\n",
    "\n",
    "fo.launch_app(dataset_yolo_lvis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
